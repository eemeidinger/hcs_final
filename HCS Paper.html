<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>HCS Paper</title>
  <link rel="stylesheet" href="https://stackedit.io/style.css" />
</head>

<body class="stackedit">
  <div class="stackedit__html"><h1 id="generating-deep-fakes-of-paleographic-writing-using-a-dcgan">Generating Deep Fakes of Paleographic Writing Using a DCGAN</h1>
<h2 id="abstract">Abstract</h2>
<p>Paleography, the study of ancient writing systems poses significant challenges for transcription due to it being extremely time intensive to work with. To help combat this issue, historians and other researchers working with paleography have looked to machine learning in hopes of expediting this process. The problem is that machine learning models often require a large amount of data to be properly trained on, and paleographic data sources are often scarce. Generative Adversarial Networks (GANS) can help circumvent this problem by generating synthetic text data. This study leverages a Deep Convolutional GAN (DCGAN) to generate deep fakes of paleographic text. This method uses both user imputed characters and an open source Chinese character dataset in order to augment the dataset provided by the users, hoping to provide more instances of paleographic text that can properly represent that which is seen in the manuscripts the user is working with. There are two parts of the DCGAN, known as the discriminator and the generator. The discriminator works to distinguish real data from synthetic data, and the generator works to fool the discriminator by creating better fake data. This process is performed iteratively until a set number of epochs, after which the generated letters are displayed and evaluated by the user to see if they properly resemble the input images. Overall, this approach showcases the potential of DCGANs in generating synthetic paleographic text, offering a promising avenue to create more data to be used in other machine learning models for transcription.</p>
<h2 id="introduction">Introduction</h2>
<p>Paleography refers to the study of ancient writing systems and the history behind them. Transcription of paleographic text, is a process that can be extremely time consuming, and can be infeasible at points due to a large amount of data and limited manpower. It is for this reason that those in the digital humanities have started to look towards machine learning models as a way to help speed up this process.</p>
<p>Historians are often attempting to look for specific keywords that are important to their research. There are multiple different avenues that digital humanists have looked to accomplish this goal. These range from simple probability search engines to comprehensive optical character recognition models. While these models have varying degrees of success for different tasks, it is typical that the most successful models are expensive in terms of both computational power and data needed. For transcription tasks, it would be ideal to have a comprehensive OCR model that you can tune for either greater precision or greater recall.</p>
<p>One of the long standing problems with using machine learning models for paleographic texts is the lack of sufficient data to train on. This is a problem for a multitude of reasons. If you want a more accurate model, you want multiple instances of very similar writing to include in your training dataset to give you a precise image of what exactly you want your word to be. This is useful for paleographic material written by one author. If you want a model with higher recall, you want a multitude of writing samples from a variety of different authors. Both of these approaches would like a good amount of training data to work with. This is where a generative adversarial network can help.</p>
<p>A generative adversarial network, or GAN, is a machine learning model that consists of two smaller models, which are the generator and the discriminator. These two models are trained simultaneously, and in this case uses binary cross-entropy as the loss function. The generator take in a single input, which is a combination of a randomly generated noise vector and a sample image of what you want to generate the deep fakes of. This input is then passed through a series of layers which transform it into replications of the original dataset. The discriminator then attempts to distinguish this generated data from the original data, and following backpropagation, the process is repeated over a set number of epochs or the model is stopped otherwise.</p>
<p>This paper means to show the process of using this form of machine learning to generate new copies of paleographic text for use in optical character recognition models.</p>
<h2 id="data">Data</h2>
<p>Figure 1[<sup>2</sup>]<br>
<img src="https://github.com/eemeidinger/hcs_final/blob/main/Screenshot%20%2897%29.png?raw=true" alt="Handwritten 9"></p>
<p>For this process, we would have two different types of data, one being the paleographic text we wish to replicate, and the other being an open source dataset containing text of the same language. . The instance we will use is the character for the number nine. While this paper is only going to show the one instance of paleographic text, it is able to be used on any version of the letter chosen. The dataset we will use to help train the model is the Chinese MNIST dataset that can be found on kaggle.[<sup>1</sup>] This dataset also has a wide varieties of handwritten forms for each number, making it able to generate copies of these numbers of sufficient variety. This is ideal when working with text with multiple authors, as it is more likely that one of the generated samples will match the one being looked for in the text.</p>
<h2 id="methods">Methods</h2>
<p>Firstly, we have to apply some basic preprocessing to both the MNIST dataset and the paleographic text. Because the MNIST dataset is not a dataset in the PyTorch library, a custom dataset function must be created to make the images compatible with the rest of the library. Following this, we subset the data to only include the labels from the target class, which is the one we want to produce the deep fakes of. After obtaining the desired subset of this dataset a series of transformations is applied to each image in order to fit the criteria for an input of the DCGAN. We do this by resizing the images to 64 by 64 pixels, setting each image to a tensor, normalizing the pixel intensities, and performing anti-aliasing to smooth out the edges of each image. The paleographic text requires a slightly different method of preprocessing, but can still be done mostly using the torch library. To start, Otsu thresholding is applied to the image as a basic form of noise removal. Following this, a linear interpolation of the text is applied to get an estimation of the letter if it was to be represented in a 64 by 64 space. Following this, we use a function from the PyTorch library called ‘<strong>to_pil_image</strong>’ to convert the tensor into a basic Python Imaging Library object (Pillow). This allows us to use the same set of transformations done to the EMNIST dataset on our input letter. Following this, we create a randomly generated noise vector to combine with the input letter, and then the model is ready to be run.</p>
<h3 id="model-architecture">Model Architecture</h3>
<p>Figure 2[<sup>3</sup>]<br>
<img src="https://miro.medium.com/v2/resize:fit:1400/1*SZo1GcSEnm4M5FQZZ2r_rA.png" alt="enter image description here"><br>
First the model randomly initializes a set of weights from a normal distribution, in this case having a mean of zero and a standard deviation of .2. After this, both the discriminator and the generator are initialized, with the generator having a hyperparameter of a latent dimension. The generator consists of layers of deconvolutional blocks followed by a final convolutional block at the end. Each deconvolutional block  consists of a transposed convolutional layer, batch normalization, and a set of activation functions. The forward propagation of the generator is where the input tensor is passed through the model to generate an output image. The discriminator has a set of convolutional blocks with a sigmoid function at the end. The forward propagation for the discriminator is where an image is passed through the model to produce a probability score indicating whether the input is real or fake. Following the initialization of these models, we move them to the GPU.</p>
<h3 id="training">Training</h3>
<p>Firstly the discriminator is trained using images from the real dataset. Real images are all labeled with the number one, and fake images are labeled with the number zero. After this the generator then creates the first set of fake images, which are then used to train the discriminator once again. The total loss of the discriminator is then calculated as: <span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>l</mi><mi>o</mi><mi>s</mi><msub><mi>s</mi><mi>d</mi></msub><mo>=</mo><mi>l</mi><mi>o</mi><mi>s</mi><msub><mi>s</mi><mi>r</mi></msub><mo>+</mo><mi>l</mi><mi>o</mi><mi>s</mi><msub><mi>s</mi><mi>f</mi></msub></mrow><annotation encoding="application/x-tex">loss_d = loss_r + loss_f</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.84444em; vertical-align: -0.15em;"></span><span class="mord mathnormal" style="margin-right: 0.01968em;">l</span><span class="mord mathnormal">os</span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.336108em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">d</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 0.84444em; vertical-align: -0.15em;"></span><span class="mord mathnormal" style="margin-right: 0.01968em;">l</span><span class="mord mathnormal">os</span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.151392em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.02778em;">r</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.980548em; vertical-align: -0.286108em;"></span><span class="mord mathnormal" style="margin-right: 0.01968em;">l</span><span class="mord mathnormal">os</span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.336108em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.10764em;">f</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.286108em;"><span class=""></span></span></span></span></span></span></span></span></span></span></span><br>
The gradients are then backpropagated and the discriminator’s parameters are updated. Following this process, the generator is trained to attempt to fool the discriminator.  The generator is zeroed out, and new coefficients are given via backpropagations. The loss for the generator is calculated based on the discriminator’s output for both the real and the fake data. After this the gradients are backpropagated and the generator’s parameters are updated. Because there is no stopping criteria, the process continues to run until the set number of epochs is completed.</p>
<h2 id="results-and-conclusion">Results and Conclusion</h2>
<p>Overall, this experiment was unsuccessful in generating new copies for the character nine. This happened for a variety of reasons. The main purpose of the DCGAN is to generate a probability density function for the pixel locations in a specified character. The way it does this is by looking at where the pixels are 255 (black) and where they are 0 (white) and calculating the probability of a certain pixel being either black or white. The DCGAN then generates copies that fall reasonably within the parameters of this probability density function. This means that the model is very dependent on the data being used to train it, as it can only create a good set of probabilities if the individual images in the training set have a relatively similar pattern. This is where the experiment began to fall apart. While each image in the train set does show the number nine, many of the images show it in different ways.<img src="https://github.com/eemeidinger/hcs_final/blob/main/Screenshot%20%2898%29.png?raw=true" alt="Image 2"><br>
<img src="https://github.com/eemeidinger/hcs_final/blob/main/Screenshot%20%2899%29.png?raw=true" alt="Image 1"><br>
<img src="https://github.com/eemeidinger/hcs_final/blob/main/Screenshot%20%28100%29.png?raw=true" alt="Image 0"><br>
When looking at these images, it is very clear that they all display the same character. However, the machine will not have nearly as easy as a time recognizing it as such for a few different reasons. Firstly, each image is a 64 x 64 image, and each letter is in a different spot in the frame. Because the machine does not know where to look, it can only recognize the image in it’s entirety. This essentially means that each image has a different pattern being represented, despite being the same character. Another issue is the size of each character in the train set. While it is intended for the overall size of the strokes to be different in order to generate a wide range of calligraphy patterns, the overall size of the characters should be relatively similar. Because the characters having different sizes, the model does not know how to properly scale the strokes to maintain consistency across the dataset, which can lead to difficulties in learning and generating coherent calligraphy patterns. These would normally be reasons to switch to a different dataset, but there is a distinct lack of machine learning datasets for Chinese characters. Overall, while the project did not turn out well this time, if there was an image dataset with better formatted data, this program could be used to generate varied copies of those images!</p>
<h2 id="references">References</h2>
<ol>
<li><a href="https://www.kaggle.com/datasets/gpreda/chinese-mnist/code">Chinese MNIST</a></li>
<li><a href="https://www.youtube.com/watch?v=kKm1QBmCYmI">Nine From YouTube</a></li>
<li><a href="https://towardsdatascience.com/deep-convolutional-gan-how-to-use-a-dcgan-to-generate-images-in-python-b08afd4d124e">DCGAN Flowchart</a></li>
</ol>
</div>
</body>

</html>
